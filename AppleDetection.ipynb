{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Detection and Yield Estimation from 3D Point Clouds\n",
    "\n",
    "## ACM SAC 2026 - Modular UAV-Based Framework\n",
    "\n",
    "[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "This notebook implements a modular framework for automated apple detection, counting, and yield estimation from UAV-derived 3D point clouds.\n",
    "\n",
    "**Key Results:**\n",
    "- Regression achieves R² ≈ 0.99 for apple count estimation\n",
    "- Clustering with MBB validation achieves R² ≈ 0.75\n",
    "- Processes 260M+ points across 50 orchard sections\n",
    "\n",
    "### Framework Configurations\n",
    "\n",
    "| Method | Color Filter | Clustering | Object Approximation | Description |\n",
    "|--------|-------------|------------|---------------------|-------------|\n",
    "| **M1** | HSV | DBSCAN | Minimum Bounding Box (MBB) | Uses cube-likeness validation |\n",
    "| **M2** | HSV | DBSCAN | Sphere | Validates clusters using sphere radius |\n",
    "| **M3** | ExR-LAB | DBSCAN | Sphere | Uses Excess Red + LAB color space filtering |\n",
    "\n",
    "### Paper Reference\n",
    "\n",
    "This code accompanies the paper: *\"A Modular UAV-Based Framework for Apple Detection and Yield Extrapolation from 3D Point Clouds\"* submitted to ACM SAC 2026.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup\n",
    "\n",
    "Run this cell to select which method to execute. You will be prompted to choose M1, M2, M3, or all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                          USER CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# INTERACTIVE METHOD SELECTION\n",
    "# Set INTERACTIVE_MODE = True to select method via input prompt\n",
    "# Set INTERACTIVE_MODE = False to use the DEFAULT_METHOD value\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "INTERACTIVE_MODE = True   # Set to False to skip the prompt and use DEFAULT_METHOD\n",
    "DEFAULT_METHOD = \"M1\"     # Used when INTERACTIVE_MODE = False\n",
    "\n",
    "def get_method_selection():\n",
    "    \"\"\"Prompt user to select a method interactively.\"\"\"\n",
    "    print(\"\")\n",
    "    print(\"=\" * 62)\n",
    "    print(\"           APPLE DETECTION METHOD SELECTION\")\n",
    "    print(\"=\" * 62)\n",
    "    print(\"  M1  : HSV + DBSCAN + MBB (Minimum Bounding Box)\")\n",
    "    print(\"  M2  : HSV + DBSCAN + Sphere\")\n",
    "    print(\"  M3  : ExR-LAB + DBSCAN + Sphere\")\n",
    "    print(\"  all : Run all methods and generate comparison\")\n",
    "    print(\"=\" * 62)\n",
    "    print(\"\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter method (M1/M2/M3/all): \").strip().upper()\n",
    "        if user_input in [\"M1\", \"M2\", \"M3\", \"ALL\"]:\n",
    "            selected = user_input if user_input != \"ALL\" else \"all\"\n",
    "            print(f\"\\n>>> Selected: {selected}\")\n",
    "            return selected\n",
    "        print(\"Invalid selection. Please enter M1, M2, M3, or all.\")\n",
    "\n",
    "# Get method selection\n",
    "if INTERACTIVE_MODE:\n",
    "    METHOD = get_method_selection()\n",
    "else:\n",
    "    METHOD = DEFAULT_METHOD\n",
    "    print(f\"Using default method: {METHOD}\")\n",
    "\n",
    "# Input/Output Paths\n",
    "INPUT_DIR = \"./data/las_files\"      # Directory containing .las point cloud files\n",
    "GT_CSV = \"./data/ground_truth.csv\"  # Ground truth CSV (columns: filename/sample, true_count/count)\n",
    "OUTPUT_BASE_DIR = \"./Results\"        # Base output directory\n",
    "\n",
    "# Training Configuration\n",
    "TRAIN_FROM_SCRATCH = False           # Set True to retrain regression models\n",
    "PRETRAINED_MODEL_DIR = \"./models\"    # Directory for pretrained models\n",
    "\n",
    "# Output Options\n",
    "SAVE_VISUALIZATIONS = True           # Generate and save storyboard visualizations\n",
    "SAVE_INTERMEDIATE_FILES = True       # Save intermediate processing files (CSV, NPY)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Method: {METHOD}\")\n",
    "print(f\"  Input directory: {INPUT_DIR}\")\n",
    "print(f\"  Ground truth CSV: {GT_CSV}\")\n",
    "print(f\"  Output directory: {OUTPUT_BASE_DIR}\")\n",
    "print(f\"  Save visualizations: {SAVE_VISUALIZATIONS}\")\n",
    "print(f\"  Save intermediate files: {SAVE_INTERMEDIATE_FILES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "import math\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Point cloud processing\n",
    "import laspy\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Color processing\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.color import rgb2lab\n",
    "\n",
    "# Coordinate transformation\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Use non-interactive backend for file saving\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Definitions\n",
    "\n",
    "All parameters from the paper (Table 1) are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HSVParams:\n",
    "    \"\"\"HSV color filtering parameters (Table 1 in paper).\"\"\"\n",
    "    H_LOW_WRAP: float = 0.96    # Hue threshold (wrap-around for red)\n",
    "    H_HIGH_WRAP: float = 0.10   # Hue upper wrap threshold\n",
    "    S_MIN: float = 0.20         # Minimum saturation\n",
    "    S_MAX: float = 0.80         # Maximum saturation\n",
    "    V_MIN: float = 0.50         # Minimum value (brightness)\n",
    "    V_MAX: float = 1.00         # Maximum value\n",
    "\n",
    "@dataclass\n",
    "class ExRLABParams:\n",
    "    \"\"\"Excess Red + LAB color filtering parameters.\"\"\"\n",
    "    RED_THRESHOLD: float = 0.20   # ExR threshold (2R - G - B)\n",
    "    A_MIN: float = 10.0           # LAB a* channel minimum\n",
    "    A_MAX: float = 60.0           # LAB a* channel maximum\n",
    "\n",
    "@dataclass\n",
    "class DBSCANParams:\n",
    "    \"\"\"DBSCAN clustering parameters.\"\"\"\n",
    "    EPS: float = 0.028           # Neighbor radius (meters)\n",
    "    MIN_SAMPLES: int = 136       # Minimum samples per cluster\n",
    "\n",
    "@dataclass\n",
    "class MBBParams:\n",
    "    \"\"\"Minimum Bounding Box validation parameters.\"\"\"\n",
    "    DIAM_MIN: float = 0.04       # Minimum space diagonal (m)\n",
    "    DIAM_MAX: float = 0.20       # Maximum space diagonal (m)\n",
    "    CUBE_RATIO_MIN: float = 0.50 # Minimum side ratio for cube-likeness\n",
    "\n",
    "@dataclass\n",
    "class SphereParams:\n",
    "    \"\"\"Sphere approximation parameters.\"\"\"\n",
    "    RADIUS_MIN: float = 0.03     # Minimum sphere radius (m)\n",
    "    RADIUS_MAX: float = 0.10     # Maximum sphere radius (m)\n",
    "\n",
    "# Instantiate default parameters\n",
    "hsv_params = HSVParams()\n",
    "exr_lab_params = ExRLABParams()\n",
    "dbscan_params = DBSCANParams()\n",
    "mbb_params = MBBParams()\n",
    "sphere_params = SphereParams()\n",
    "\n",
    "# Coordinate system handling\n",
    "FORCE_LONLAT = None  # None=auto-detect, True=force, False=skip\n",
    "\n",
    "print(\"Parameters initialized (from Table 1 in paper):\")\n",
    "print(f\"  HSV: H in [0, {hsv_params.H_HIGH_WRAP}] or [{hsv_params.H_LOW_WRAP}, 1]\")\n",
    "print(f\"       S in [{hsv_params.S_MIN}, {hsv_params.S_MAX}], V in [{hsv_params.V_MIN}, {hsv_params.V_MAX}]\")\n",
    "print(f\"  DBSCAN: eps={dbscan_params.EPS}m, min_samples={dbscan_params.MIN_SAMPLES}\")\n",
    "print(f\"  MBB: D in [{mbb_params.DIAM_MIN}, {mbb_params.DIAM_MAX}]m, cube_ratio >= {mbb_params.CUBE_RATIO_MIN}\")\n",
    "print(f\"  Sphere: R in [{sphere_params.RADIUS_MIN}, {sphere_params.RADIUS_MAX}]m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions\n",
    "\n",
    "Core utility functions for file I/O, coordinate transformation, and data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                          FILE I/O FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def read_las_xyzrgb(las_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read LAS file and extract XYZ coordinates with normalized RGB values.\n",
    "    \n",
    "    Args:\n",
    "        las_path: Path to .las file\n",
    "        \n",
    "    Returns:\n",
    "        xyz: (N, 3) array of coordinates\n",
    "        rgb: (N, 3) array of RGB values in [0, 1]\n",
    "    \"\"\"\n",
    "    las = laspy.read(las_path)\n",
    "    xyz = np.vstack((las.x, las.y, las.z)).T\n",
    "    \n",
    "    if hasattr(las, \"red\") and hasattr(las, \"green\") and hasattr(las, \"blue\"):\n",
    "        r = np.asarray(las.red, dtype=np.float32)\n",
    "        g = np.asarray(las.green, dtype=np.float32)\n",
    "        b = np.asarray(las.blue, dtype=np.float32)\n",
    "        \n",
    "        # Auto-detect 8-bit vs 16-bit color encoding\n",
    "        max_val = 65535.0 if max(r.max(initial=0), g.max(initial=0), b.max(initial=0)) > 255 else 255.0\n",
    "        rgb = np.vstack((r, g, b)).T / max_val\n",
    "        rgb = np.clip(rgb, 0.0, 1.0)\n",
    "    else:\n",
    "        rgb = np.zeros((xyz.shape[0], 3), dtype=np.float32)\n",
    "    \n",
    "    return xyz, rgb\n",
    "\n",
    "\n",
    "def load_ground_truth(gt_csv_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Load ground truth apple counts from CSV.\n",
    "    \n",
    "    Accepts column formats:\n",
    "      - (filename, true_count)\n",
    "      - (sample, count)\n",
    "    \n",
    "    Args:\n",
    "        gt_csv_path: Path to ground truth CSV\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping sample keys (lowercase, no extension) to counts\n",
    "    \"\"\"\n",
    "    if not os.path.exists(gt_csv_path):\n",
    "        print(f\"Warning: Ground truth CSV not found: {gt_csv_path}\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(gt_csv_path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    if {\"filename\", \"true_count\"}.issubset(cols.keys()):\n",
    "        df = df.rename(columns={cols[\"filename\"]: \"filename\", cols[\"true_count\"]: \"true_count\"})\n",
    "    elif {\"sample\", \"count\"}.issubset(cols.keys()):\n",
    "        df = df.rename(columns={cols[\"sample\"]: \"filename\", cols[\"count\"]: \"true_count\"})\n",
    "    else:\n",
    "        available_cols = list(df.columns)\n",
    "        raise ValueError(f\"GT CSV must have (filename, true_count) or (sample, count). Found: {available_cols}\")\n",
    "    \n",
    "    df[\"key\"] = df[\"filename\"].astype(str).str.lower().str.replace(r\"\\.[^.]+$\", \"\", regex=True)\n",
    "    df[\"true_count\"] = pd.to_numeric(df[\"true_count\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"key\", \"true_count\"])\n",
    "    \n",
    "    gt_map = dict(zip(df[\"key\"], df[\"true_count\"]))\n",
    "    print(f\"Loaded ground truth for {len(gt_map)} samples\")\n",
    "    return gt_map\n",
    "\n",
    "\n",
    "def setup_output_directories(base_dir: str, method: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create standardized output directory structure.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base output directory\n",
    "        method: Method name (M1, M2, M3, or Combined)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of output paths\n",
    "    \"\"\"\n",
    "    paths = {\n",
    "        \"root\": os.path.join(base_dir, method),\n",
    "        \"intermediate\": os.path.join(base_dir, method, \"intermediate\"),\n",
    "        \"visualizations\": os.path.join(base_dir, method, \"visualizations\"),\n",
    "        \"metrics\": os.path.join(base_dir, method, \"metrics\"),\n",
    "        \"models\": os.path.join(base_dir, method, \"models\"),\n",
    "    }\n",
    "    \n",
    "    for path in paths.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "print(\"File I/O functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    COORDINATE TRANSFORMATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def looks_like_lonlat(xy: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if coordinates are in longitude/latitude degrees.\n",
    "    \"\"\"\n",
    "    x, y = xy[:, 0], xy[:, 1]\n",
    "    if not (np.all(np.isfinite(x)) and np.all(np.isfinite(y))):\n",
    "        return False\n",
    "    \n",
    "    in_range = (x.min() >= -180) and (x.max() <= 180) and (y.min() >= -90) and (y.max() <= 90)\n",
    "    span_ok = (x.max() - x.min()) < 2.0 and (y.max() - y.min()) < 2.0\n",
    "    return in_range and span_ok\n",
    "\n",
    "\n",
    "def utm_epsg_from_lonlat(lon: float, lat: float) -> str:\n",
    "    \"\"\"\n",
    "    Determine UTM EPSG code from geographic coordinates.\n",
    "    \"\"\"\n",
    "    zone = int(np.floor((lon + 180) / 6) + 1)\n",
    "    return f\"EPSG:326{zone:02d}\" if lat >= 0 else f\"EPSG:327{zone:02d}\"\n",
    "\n",
    "\n",
    "def reproject_xy_to_meters(xy: np.ndarray, force_lonlat: Optional[bool] = None) -> Tuple[np.ndarray, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Reproject XY coordinates to meters if needed.\n",
    "    \"\"\"\n",
    "    if force_lonlat is False:\n",
    "        return xy.copy(), None\n",
    "    \n",
    "    if force_lonlat is True or looks_like_lonlat(xy):\n",
    "        lon, lat = xy[:, 0], xy[:, 1]\n",
    "        epsg = utm_epsg_from_lonlat(float(np.mean(lon)), float(np.mean(lat)))\n",
    "        \n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", epsg, always_xy=True)\n",
    "        x_m, y_m = transformer.transform(lon, lat)\n",
    "        xy_m = np.column_stack([x_m, y_m])\n",
    "        \n",
    "        return xy_m, epsg\n",
    "    else:\n",
    "        return xy.copy(), None\n",
    "\n",
    "print(\"Coordinate transformation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Color Filtering Functions\n",
    "\n",
    "Implementation of HSV and ExR-LAB color filtering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                         COLOR FILTERING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def hsv_red_mask(rgb: np.ndarray, params: HSVParams = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    HSV-based red color filtering for apple detection.\n",
    "    The hue component wraps around 0/1 for red colors.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = hsv_params\n",
    "    \n",
    "    hsv = mcolors.rgb_to_hsv(rgb)\n",
    "    h, s, v = hsv[:, 0], hsv[:, 1], hsv[:, 2]\n",
    "    \n",
    "    hue_mask = (h >= params.H_LOW_WRAP) | (h <= params.H_HIGH_WRAP)\n",
    "    sat_mask = (s >= params.S_MIN) & (s <= params.S_MAX)\n",
    "    val_mask = (v >= params.V_MIN) & (v <= params.V_MAX)\n",
    "    \n",
    "    return hue_mask & sat_mask & val_mask\n",
    "\n",
    "\n",
    "def excess_red_mask(rgb: np.ndarray, params: ExRLABParams = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Excess Red + LAB color filtering for apple detection.\n",
    "    Two-stage: ExR = 2*R - G - B, then LAB a* channel constraint.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = exr_lab_params\n",
    "    \n",
    "    excess_red = 2.0 * rgb[:, 0] - rgb[:, 1] - rgb[:, 2]\n",
    "    \n",
    "    lab = rgb2lab(rgb.reshape(-1, 1, 3)).reshape(-1, 3)\n",
    "    a_channel = lab[:, 1]\n",
    "    \n",
    "    return (excess_red > params.RED_THRESHOLD) & \\\n",
    "           (a_channel >= params.A_MIN) & \\\n",
    "           (a_channel <= params.A_MAX)\n",
    "\n",
    "print(\"Color filtering functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Object Approximation Functions\n",
    "\n",
    "Implementation of MBB and Sphere geometric approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                      OBJECT APPROXIMATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_aabb(points: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute Axis-Aligned Bounding Box for a point cluster.\"\"\"\n",
    "    min_pt = points.min(axis=0)\n",
    "    max_pt = points.max(axis=0)\n",
    "    sides = max_pt - min_pt\n",
    "    return min_pt, max_pt, sides\n",
    "\n",
    "\n",
    "def validate_mbb(sides: np.ndarray, params: MBBParams = None) -> Tuple[bool, Dict[str, float]]:\n",
    "    \"\"\"Validate cluster using Minimum Bounding Box criteria (cube-likeness + size).\"\"\"\n",
    "    if params is None:\n",
    "        params = mbb_params\n",
    "    \n",
    "    dx, dy, dz = float(sides[0]), float(sides[1]), float(sides[2])\n",
    "    space_diagonal = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    \n",
    "    s_min = min(dx, dy, dz)\n",
    "    s_max = max(dx, dy, dz)\n",
    "    cube_ratio = s_min / s_max if s_max > 0 else 0\n",
    "    \n",
    "    is_cubelike = cube_ratio >= params.CUBE_RATIO_MIN\n",
    "    in_size_window = params.DIAM_MIN <= space_diagonal <= params.DIAM_MAX\n",
    "    is_valid = is_cubelike and in_size_window\n",
    "    \n",
    "    metrics = {\n",
    "        \"dx\": dx, \"dy\": dy, \"dz\": dz,\n",
    "        \"space_diagonal\": space_diagonal,\n",
    "        \"cube_ratio\": cube_ratio,\n",
    "        \"is_cubelike\": is_cubelike,\n",
    "        \"in_size_window\": in_size_window,\n",
    "    }\n",
    "    \n",
    "    return is_valid, metrics\n",
    "\n",
    "\n",
    "def validate_sphere(sides: np.ndarray, params: SphereParams = None) -> Tuple[bool, Dict[str, float]]:\n",
    "    \"\"\"Validate cluster using inscribed sphere criteria.\"\"\"\n",
    "    if params is None:\n",
    "        params = sphere_params\n",
    "    \n",
    "    min_side = float(np.min(sides))\n",
    "    radius = min_side / 2.0\n",
    "    volume = (4.0 / 3.0) * np.pi * (radius ** 3)\n",
    "    \n",
    "    in_size_window = params.RADIUS_MIN <= radius <= params.RADIUS_MAX\n",
    "    is_valid = in_size_window\n",
    "    \n",
    "    metrics = {\n",
    "        \"radius\": radius,\n",
    "        \"diameter\": min_side,\n",
    "        \"volume\": volume,\n",
    "        \"in_size_window\": in_size_window,\n",
    "    }\n",
    "    \n",
    "    return is_valid, metrics\n",
    "\n",
    "\n",
    "def estimate_ellipsoid_volume(sides: np.ndarray, alpha: float = 1.15) -> float:\n",
    "    \"\"\"Estimate apple volume using prolate ellipsoid model.\"\"\"\n",
    "    dx, dy, dz = float(sides[0]), float(sides[1]), float(sides[2])\n",
    "    a = min(dx, dy) / 2.0\n",
    "    c = min(alpha * a, dz / 2.0)\n",
    "    return (4.0 / 3.0) * np.pi * (a ** 2) * c\n",
    "\n",
    "print(\"Object approximation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Functions\n",
    "\n",
    "Functions to generate storyboard visualizations of detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                         VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_storyboard(sample_id: str, xyz: np.ndarray, rgb: np.ndarray,\n",
    "                        red_xyz: np.ndarray, labels: np.ndarray,\n",
    "                        accepted_mask: np.ndarray, output_path: str,\n",
    "                        method_name: str, ground_truth: Optional[float] = None):\n",
    "    \"\"\"\n",
    "    Generate a 4-panel storyboard visualization.\n",
    "    \n",
    "    Panels:\n",
    "    1. Original point cloud (RGB)\n",
    "    2. Red-filtered points\n",
    "    3. DBSCAN clusters (colored by label)\n",
    "    4. Accepted clusters only (valid apples)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Subsample for faster plotting\n",
    "    max_points = 50000\n",
    "    if xyz.shape[0] > max_points:\n",
    "        idx = np.random.choice(xyz.shape[0], max_points, replace=False)\n",
    "        xyz_sub, rgb_sub = xyz[idx], rgb[idx]\n",
    "    else:\n",
    "        xyz_sub, rgb_sub = xyz, rgb\n",
    "    \n",
    "    # Panel 1: Original point cloud\n",
    "    ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "    ax1.scatter(xyz_sub[:, 0], xyz_sub[:, 1], xyz_sub[:, 2], \n",
    "                c=rgb_sub, s=0.1, alpha=0.5)\n",
    "    ax1.set_title(f\"1. Original Point Cloud (N={xyz.shape[0]:,})\")\n",
    "    ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.set_zlabel('Z')\n",
    "    \n",
    "    # Panel 2: Red-filtered points\n",
    "    ax2 = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "    if red_xyz.shape[0] > 0:\n",
    "        red_sub = red_xyz if red_xyz.shape[0] <= max_points else red_xyz[np.random.choice(red_xyz.shape[0], max_points, replace=False)]\n",
    "        ax2.scatter(red_sub[:, 0], red_sub[:, 1], red_sub[:, 2], \n",
    "                    c='red', s=0.5, alpha=0.6)\n",
    "    ax2.set_title(f\"2. Red-Filtered Points (N={red_xyz.shape[0]:,})\")\n",
    "    ax2.set_xlabel('X'); ax2.set_ylabel('Y'); ax2.set_zlabel('Z')\n",
    "    \n",
    "    # Panel 3: DBSCAN clusters\n",
    "    ax3 = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "    unique_labels = np.unique(labels[labels >= 0])\n",
    "    n_clusters = len(unique_labels)\n",
    "    if n_clusters > 0:\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, max(n_clusters, 1)))\n",
    "        for i, lab in enumerate(unique_labels):\n",
    "            pts = red_xyz[labels == lab]\n",
    "            if pts.shape[0] > 0:\n",
    "                ax3.scatter(pts[:, 0], pts[:, 1], pts[:, 2], \n",
    "                           c=[colors[i % len(colors)]], s=1, alpha=0.7)\n",
    "    ax3.set_title(f\"3. DBSCAN Clusters (N={n_clusters})\")\n",
    "    ax3.set_xlabel('X'); ax3.set_ylabel('Y'); ax3.set_zlabel('Z')\n",
    "    \n",
    "    # Panel 4: Accepted clusters\n",
    "    ax4 = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "    n_accepted = accepted_mask.sum()\n",
    "    accepted_labels = unique_labels[accepted_mask[:len(unique_labels)]] if len(accepted_mask) >= len(unique_labels) else []\n",
    "    if len(accepted_labels) > 0:\n",
    "        for i, lab in enumerate(accepted_labels):\n",
    "            pts = red_xyz[labels == lab]\n",
    "            if pts.shape[0] > 0:\n",
    "                ax4.scatter(pts[:, 0], pts[:, 1], pts[:, 2], \n",
    "                           c='green', s=2, alpha=0.8)\n",
    "    gt_str = f\", GT={int(ground_truth)}\" if ground_truth else \"\"\n",
    "    ax4.set_title(f\"4. Accepted Apples (N={n_accepted}{gt_str})\")\n",
    "    ax4.set_xlabel('X'); ax4.set_ylabel('Y'); ax4.set_zlabel('Z')\n",
    "    \n",
    "    plt.suptitle(f\"{sample_id} - {method_name} Detection Pipeline\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def generate_comparison_plot(all_results: Dict, output_dir: str):\n",
    "    \"\"\"Generate comparison bar charts and scatter plots for all methods.\"\"\"\n",
    "    methods = list(all_results.keys())\n",
    "    \n",
    "    # Metrics comparison bar chart\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    # RMSE\n",
    "    rmse_vals = [all_results[m][\"metrics\"].get(\"rmse\", 0) for m in methods]\n",
    "    axes[0].bar(methods, rmse_vals, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    axes[0].set_title(\"RMSE (lower is better)\")\n",
    "    axes[0].set_ylabel(\"RMSE\")\n",
    "    \n",
    "    # R2\n",
    "    r2_vals = [all_results[m][\"metrics\"].get(\"r2\", 0) for m in methods]\n",
    "    axes[1].bar(methods, r2_vals, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    axes[1].set_title(\"R² Score (higher is better)\")\n",
    "    axes[1].set_ylabel(\"R²\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    # Mean Relative Error\n",
    "    mre_vals = [all_results[m][\"metrics\"].get(\"mean_rel_error\", 0) for m in methods]\n",
    "    axes[2].bar(methods, mre_vals, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    axes[2].set_title(\"Mean Relative Error (lower is better)\")\n",
    "    axes[2].set_ylabel(\"MRE\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"method_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Predicted vs Actual scatter plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        results = all_results[method][\"results\"]\n",
    "        valid = [(r.detected_count, r.ground_truth) for r in results \n",
    "                 if r.ground_truth is not None and np.isfinite(r.ground_truth)]\n",
    "        if valid:\n",
    "            pred, actual = zip(*valid)\n",
    "            axes[i].scatter(actual, pred, c=colors[i], alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "            max_val = max(max(pred), max(actual)) * 1.1\n",
    "            axes[i].plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='Perfect')\n",
    "            axes[i].set_xlabel('Ground Truth')\n",
    "            axes[i].set_ylabel('Predicted')\n",
    "            axes[i].set_title(f\"{method} (R²={all_results[method]['metrics'].get('r2', 0):.3f})\")\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"predicted_vs_actual.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Core Detection Pipeline\n",
    "\n",
    "Main detection pipeline with intermediate file saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                         CORE DETECTION PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    \"\"\"Container for detection results from a single file.\"\"\"\n",
    "    sample_id: str\n",
    "    n_total_points: int\n",
    "    n_red_points: int\n",
    "    n_clusters: int\n",
    "    n_accepted_clusters: int\n",
    "    detected_count: int\n",
    "    ground_truth: Optional[float]\n",
    "    cluster_metrics: List[Dict]\n",
    "    total_volume: float\n",
    "    # For visualization\n",
    "    xyz: Optional[np.ndarray] = None\n",
    "    rgb: Optional[np.ndarray] = None\n",
    "    red_xyz: Optional[np.ndarray] = None\n",
    "    labels: Optional[np.ndarray] = None\n",
    "    accepted_mask: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "def detect_apples_single_file(\n",
    "    las_path: str,\n",
    "    color_method: str = \"HSV\",\n",
    "    validation_method: str = \"MBB\",\n",
    "    gt_map: Optional[Dict[str, float]] = None,\n",
    "    keep_arrays: bool = False\n",
    ") -> DetectionResult:\n",
    "    \"\"\"\n",
    "    Run apple detection pipeline on a single LAS file.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load LAS file (XYZ + RGB)\n",
    "    2. Reproject to meters if needed\n",
    "    3. Apply color filtering (HSV or ExR-LAB)\n",
    "    4. Run DBSCAN clustering\n",
    "    5. Validate clusters using geometric approximation (MBB or Sphere)\n",
    "    6. Count accepted clusters as detected apples\n",
    "    \"\"\"\n",
    "    sample_id = os.path.splitext(os.path.basename(las_path))[0].lower()\n",
    "    \n",
    "    # Load point cloud\n",
    "    xyz, rgb = read_las_xyzrgb(las_path)\n",
    "    n_total_points = xyz.shape[0]\n",
    "    \n",
    "    # Reproject to meters\n",
    "    xy_m, _ = reproject_xy_to_meters(xyz[:, :2], FORCE_LONLAT)\n",
    "    xyz_m = np.column_stack([xy_m, xyz[:, 2]])\n",
    "    \n",
    "    # Apply color filtering\n",
    "    if color_method.upper() == \"HSV\":\n",
    "        red_mask = hsv_red_mask(rgb)\n",
    "    else:\n",
    "        red_mask = excess_red_mask(rgb)\n",
    "    \n",
    "    red_xyz = xyz_m[red_mask]\n",
    "    n_red_points = red_xyz.shape[0]\n",
    "    \n",
    "    # Handle empty case\n",
    "    if n_red_points == 0:\n",
    "        return DetectionResult(\n",
    "            sample_id=sample_id, n_total_points=n_total_points, n_red_points=0,\n",
    "            n_clusters=0, n_accepted_clusters=0, detected_count=0,\n",
    "            ground_truth=gt_map.get(sample_id) if gt_map else None,\n",
    "            cluster_metrics=[], total_volume=0.0,\n",
    "            xyz=xyz_m if keep_arrays else None, rgb=rgb if keep_arrays else None,\n",
    "            red_xyz=red_xyz if keep_arrays else None, labels=np.array([]) if keep_arrays else None,\n",
    "            accepted_mask=np.array([]) if keep_arrays else None\n",
    "        )\n",
    "    \n",
    "    # DBSCAN clustering\n",
    "    labels = DBSCAN(eps=dbscan_params.EPS, min_samples=dbscan_params.MIN_SAMPLES).fit_predict(red_xyz)\n",
    "    unique_labels = [l for l in np.unique(labels) if l != -1]\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    # Validate clusters\n",
    "    cluster_metrics = []\n",
    "    accepted_list = []\n",
    "    detected_count = 0\n",
    "    total_volume = 0.0\n",
    "    \n",
    "    for lab in unique_labels:\n",
    "        pts = red_xyz[labels == lab]\n",
    "        if pts.shape[0] < 2:\n",
    "            accepted_list.append(False)\n",
    "            continue\n",
    "        \n",
    "        min_pt, max_pt, sides = compute_aabb(pts)\n",
    "        centroid = pts.mean(axis=0)\n",
    "        \n",
    "        if validation_method.upper() == \"MBB\":\n",
    "            is_valid, metrics = validate_mbb(sides)\n",
    "        else:\n",
    "            is_valid, metrics = validate_sphere(sides)\n",
    "        \n",
    "        metrics[\"cluster_id\"] = int(lab)\n",
    "        metrics[\"n_points\"] = pts.shape[0]\n",
    "        metrics[\"centroid_x\"] = float(centroid[0])\n",
    "        metrics[\"centroid_y\"] = float(centroid[1])\n",
    "        metrics[\"centroid_z\"] = float(centroid[2])\n",
    "        metrics[\"accepted\"] = is_valid\n",
    "        cluster_metrics.append(metrics)\n",
    "        accepted_list.append(is_valid)\n",
    "        \n",
    "        if is_valid:\n",
    "            detected_count += 1\n",
    "            total_volume += estimate_ellipsoid_volume(sides)\n",
    "    \n",
    "    return DetectionResult(\n",
    "        sample_id=sample_id, n_total_points=n_total_points, n_red_points=n_red_points,\n",
    "        n_clusters=n_clusters, n_accepted_clusters=detected_count, detected_count=detected_count,\n",
    "        ground_truth=gt_map.get(sample_id) if gt_map else None,\n",
    "        cluster_metrics=cluster_metrics, total_volume=total_volume,\n",
    "        xyz=xyz_m if keep_arrays else None, rgb=rgb if keep_arrays else None,\n",
    "        red_xyz=red_xyz if keep_arrays else None, labels=labels if keep_arrays else None,\n",
    "        accepted_mask=np.array(accepted_list) if keep_arrays else None\n",
    "    )\n",
    "\n",
    "print(\"Core detection pipeline defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Method Runner Functions\n",
    "\n",
    "Functions to run M1, M2, M3, and combined analysis with full output saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                          METHOD RUNNER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(results: List[DetectionResult]) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics from detection results.\"\"\"\n",
    "    valid_results = [r for r in results if r.ground_truth is not None and np.isfinite(r.ground_truth)]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return {\"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan, \"mean_rel_error\": np.nan, \"n_samples\": 0}\n",
    "    \n",
    "    y_true = np.array([r.ground_truth for r in valid_results])\n",
    "    y_pred = np.array([r.detected_count for r in valid_results])\n",
    "    \n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    r2 = float(r2_score(y_true, y_pred)) if np.var(y_true) > 0 else np.nan\n",
    "    \n",
    "    rel_errors = [abs(r.detected_count - r.ground_truth) / r.ground_truth \n",
    "                  for r in valid_results if r.ground_truth > 0]\n",
    "    mean_rel_error = float(np.mean(rel_errors)) if rel_errors else np.nan\n",
    "    \n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"mean_rel_error\": mean_rel_error, \"n_samples\": len(valid_results)}\n",
    "\n",
    "\n",
    "def save_intermediate_files(result: DetectionResult, paths: Dict[str, str], method_name: str):\n",
    "    \"\"\"Save intermediate processing files for a single sample.\"\"\"\n",
    "    sample_id = result.sample_id\n",
    "    intermediate_dir = paths[\"intermediate\"]\n",
    "    \n",
    "    # Save cluster metrics as CSV\n",
    "    if result.cluster_metrics:\n",
    "        cluster_df = pd.DataFrame(result.cluster_metrics)\n",
    "        cluster_df.to_csv(os.path.join(intermediate_dir, f\"{sample_id}_clusters.csv\"), index=False)\n",
    "    \n",
    "    # Save red points as NPY (compressed)\n",
    "    if result.red_xyz is not None and result.red_xyz.shape[0] > 0:\n",
    "        np.savez_compressed(\n",
    "            os.path.join(intermediate_dir, f\"{sample_id}_filtered.npz\"),\n",
    "            red_xyz=result.red_xyz,\n",
    "            labels=result.labels if result.labels is not None else np.array([])\n",
    "        )\n",
    "\n",
    "\n",
    "def run_method(method_name: str, input_dir: str, gt_csv: str, output_base: str,\n",
    "               save_viz: bool = True, save_intermediate: bool = True) -> Tuple[List[DetectionResult], Dict]:\n",
    "    \"\"\"\n",
    "    Run a specific detection method on all LAS files.\n",
    "    Saves results to metrics/, intermediate/, and visualizations/ folders.\n",
    "    \"\"\"\n",
    "    method_config = {\n",
    "        \"M1\": {\"color\": \"HSV\", \"validation\": \"MBB\"},\n",
    "        \"M2\": {\"color\": \"HSV\", \"validation\": \"Sphere\"},\n",
    "        \"M3\": {\"color\": \"ExR-LAB\", \"validation\": \"Sphere\"},\n",
    "    }\n",
    "    \n",
    "    if method_name not in method_config:\n",
    "        raise ValueError(f\"Unknown method: {method_name}. Choose from M1, M2, M3.\")\n",
    "    \n",
    "    config = method_config[method_name]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {method_name}: {config['color']} + DBSCAN + {config['validation']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    paths = setup_output_directories(output_base, method_name)\n",
    "    gt_map = load_ground_truth(gt_csv) if os.path.exists(gt_csv) else {}\n",
    "    \n",
    "    las_files = sorted(glob.glob(os.path.join(input_dir, \"*.las\")))\n",
    "    if not las_files:\n",
    "        print(f\"No LAS files found in {input_dir}\")\n",
    "        return [], {}\n",
    "    \n",
    "    print(f\"Found {len(las_files)} LAS files\")\n",
    "    keep_arrays = save_viz or save_intermediate\n",
    "    \n",
    "    results = []\n",
    "    for i, las_path in enumerate(las_files):\n",
    "        result = detect_apples_single_file(\n",
    "            las_path, color_method=config[\"color\"], validation_method=config[\"validation\"],\n",
    "            gt_map=gt_map, keep_arrays=keep_arrays\n",
    "        )\n",
    "        results.append(result)\n",
    "        \n",
    "        gt_str = f\"GT={int(result.ground_truth)}\" if result.ground_truth else \"GT=N/A\"\n",
    "        print(f\"  [{i+1:3d}/{len(las_files)}] {result.sample_id}: detected={result.detected_count}, {gt_str}\")\n",
    "        \n",
    "        # Save intermediate files\n",
    "        if save_intermediate and result.red_xyz is not None:\n",
    "            save_intermediate_files(result, paths, method_name)\n",
    "        \n",
    "        # Generate visualization\n",
    "        if save_viz and result.xyz is not None and result.red_xyz is not None:\n",
    "            viz_path = os.path.join(paths[\"visualizations\"], f\"{result.sample_id}_storyboard.png\")\n",
    "            try:\n",
    "                generate_storyboard(\n",
    "                    result.sample_id, result.xyz, result.rgb, result.red_xyz,\n",
    "                    result.labels if result.labels is not None else np.array([]),\n",
    "                    result.accepted_mask if result.accepted_mask is not None else np.array([]),\n",
    "                    viz_path, method_name, result.ground_truth\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: Could not generate visualization: {e}\")\n",
    "        \n",
    "        # Clear arrays to save memory\n",
    "        result.xyz = None\n",
    "        result.rgb = None\n",
    "        result.red_xyz = None\n",
    "        result.labels = None\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(results)\n",
    "    \n",
    "    print(f\"\\n{method_name} Results:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.3f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.3f}\")\n",
    "    print(f\"  R2: {metrics['r2']:.4f}\")\n",
    "    print(f\"  Mean Relative Error: {metrics['mean_rel_error']:.3f}\")\n",
    "    \n",
    "    # Save results CSV\n",
    "    results_df = pd.DataFrame([{\n",
    "        \"sample\": r.sample_id, \"predicted\": r.detected_count, \"ground_truth\": r.ground_truth,\n",
    "        \"n_red_points\": r.n_red_points, \"n_clusters\": r.n_clusters,\n",
    "        \"n_accepted\": r.n_accepted_clusters, \"total_volume_m3\": r.total_volume,\n",
    "    } for r in results])\n",
    "    results_csv = os.path.join(paths[\"metrics\"], f\"{method_name}_results.csv\")\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    \n",
    "    # Save summary CSV\n",
    "    summary_df = pd.DataFrame([metrics])\n",
    "    summary_csv = os.path.join(paths[\"metrics\"], f\"{method_name}_summary.csv\")\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    \n",
    "    # Save metadata JSON\n",
    "    metadata = {\n",
    "        \"method\": method_name, \"color_filter\": config[\"color\"], \"validation\": config[\"validation\"],\n",
    "        \"timestamp\": datetime.now().isoformat(), \"n_files\": len(las_files),\n",
    "        \"parameters\": {\n",
    "            \"dbscan_eps\": dbscan_params.EPS, \"dbscan_min_samples\": dbscan_params.MIN_SAMPLES,\n",
    "            \"hsv_params\": asdict(hsv_params), \"mbb_params\": asdict(mbb_params), \"sphere_params\": asdict(sphere_params)\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(paths[\"metrics\"], f\"{method_name}_metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSaved outputs to: {paths['root']}\")\n",
    "    print(f\"  - metrics/{method_name}_results.csv\")\n",
    "    print(f\"  - metrics/{method_name}_summary.csv\")\n",
    "    if save_intermediate:\n",
    "        print(f\"  - intermediate/ ({len(las_files)} files)\")\n",
    "    if save_viz:\n",
    "        print(f\"  - visualizations/ ({len(las_files)} storyboards)\")\n",
    "    \n",
    "    return results, metrics\n",
    "\n",
    "\n",
    "def run_M1(input_dir=INPUT_DIR, gt_csv=GT_CSV, output_base=OUTPUT_BASE_DIR):\n",
    "    \"\"\"Run M1: HSV + DBSCAN + MBB\"\"\"\n",
    "    return run_method(\"M1\", input_dir, gt_csv, output_base, SAVE_VISUALIZATIONS, SAVE_INTERMEDIATE_FILES)\n",
    "\n",
    "def run_M2(input_dir=INPUT_DIR, gt_csv=GT_CSV, output_base=OUTPUT_BASE_DIR):\n",
    "    \"\"\"Run M2: HSV + DBSCAN + Sphere\"\"\"\n",
    "    return run_method(\"M2\", input_dir, gt_csv, output_base, SAVE_VISUALIZATIONS, SAVE_INTERMEDIATE_FILES)\n",
    "\n",
    "def run_M3(input_dir=INPUT_DIR, gt_csv=GT_CSV, output_base=OUTPUT_BASE_DIR):\n",
    "    \"\"\"Run M3: ExR-LAB + DBSCAN + Sphere\"\"\"\n",
    "    return run_method(\"M3\", input_dir, gt_csv, output_base, SAVE_VISUALIZATIONS, SAVE_INTERMEDIATE_FILES)\n",
    "\n",
    "def run_all(input_dir=INPUT_DIR, gt_csv=GT_CSV, output_base=OUTPUT_BASE_DIR):\n",
    "    \"\"\"Run all methods and generate combined comparison.\"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for method in [\"M1\", \"M2\", \"M3\"]:\n",
    "        results, metrics = run_method(method, input_dir, gt_csv, output_base, \n",
    "                                       SAVE_VISUALIZATIONS, SAVE_INTERMEDIATE_FILES)\n",
    "        all_results[method] = {\"results\": results, \"metrics\": metrics}\n",
    "    \n",
    "    # Generate combined outputs\n",
    "    combined_dir = os.path.join(output_base, \"Combined\")\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "    \n",
    "    # Comparison CSV\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\"Method\": method, **data[\"metrics\"]}\n",
    "        for method, data in all_results.items()\n",
    "    ])\n",
    "    comparison_df.to_csv(os.path.join(combined_dir, \"method_comparison.csv\"), index=False)\n",
    "    \n",
    "    # Comparison plots\n",
    "    generate_comparison_plot(all_results, combined_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Method Comparison Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(f\"\\nSaved to: {combined_dir}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"Method runner functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Regression Training (Optional)\n",
    "\n",
    "Train regression models to predict apple count from red-point features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                       REGRESSION MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def train_regression_models(results: List[DetectionResult], output_dir: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Train regression models to predict apple count from clustering features.\n",
    "    Uses features: n_red_points, n_clusters, n_accepted_clusters, total_volume\n",
    "    \"\"\"\n",
    "    valid = [r for r in results if r.ground_truth is not None and np.isfinite(r.ground_truth)]\n",
    "    \n",
    "    if len(valid) < 5:\n",
    "        print(\"Not enough samples for training (need at least 5)\")\n",
    "        return {}\n",
    "    \n",
    "    # Prepare features\n",
    "    X = np.array([[r.n_red_points, r.n_clusters, r.n_accepted_clusters, r.total_volume] for r in valid])\n",
    "    y = np.array([r.ground_truth for r in valid])\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Models to train\n",
    "    models = {\n",
    "        \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=min(5, len(X_train)))\n",
    "    }\n",
    "    \n",
    "    trained = {}\n",
    "    print(\"\\nTraining regression models:\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        print(f\"  {name}: R²={r2:.4f}, RMSE={rmse:.3f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, f\"regression_{name.lower()}.pkl\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        trained[name] = {\"model\": model, \"r2\": r2, \"rmse\": rmse, \"path\": model_path}\n",
    "    \n",
    "    print(f\"\\nModels saved to: {output_dir}\")\n",
    "    return trained\n",
    "\n",
    "print(\"Regression training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Execution\n",
    "\n",
    "Run the selected method based on the configuration in Cell 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                            MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution based on METHOD selection.\"\"\"\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# Apple Detection Framework - ACM SAC 2026\")\n",
    "    print(f\"# Selected Method: {METHOD}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"ERROR: Input directory does not exist: {INPUT_DIR}\")\n",
    "        print(\"Please update INPUT_DIR in Cell 1 or create the directory.\")\n",
    "        return\n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    if METHOD.upper() == \"M1\":\n",
    "        results, _ = run_M1()\n",
    "    elif METHOD.upper() == \"M2\":\n",
    "        results, _ = run_M2()\n",
    "    elif METHOD.upper() == \"M3\":\n",
    "        results, _ = run_M3()\n",
    "    elif METHOD.upper() == \"ALL\":\n",
    "        all_results = run_all()\n",
    "        results = all_results.get(\"M1\", {}).get(\"results\", [])\n",
    "    else:\n",
    "        print(f\"Unknown method: {METHOD}\")\n",
    "        print(\"Valid options: M1, M2, M3, all\")\n",
    "        return\n",
    "    \n",
    "    # Optional: Train regression models\n",
    "    if TRAIN_FROM_SCRATCH and results:\n",
    "        models_dir = os.path.join(OUTPUT_BASE_DIR, METHOD.upper() if METHOD.upper() != \"ALL\" else \"M1\", \"models\")\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        train_regression_models(results, models_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Results saved to: {OUTPUT_BASE_DIR}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Verification\n",
    "\n",
    "Verify that all components are properly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Framework Verification:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Color filters:     hsv_red_mask, excess_red_mask\")\n",
    "print(\"Validation:        validate_mbb, validate_sphere\")\n",
    "print(\"Pipeline:          detect_apples_single_file\")\n",
    "print(\"Methods:           run_M1, run_M2, run_M3, run_all\")\n",
    "print(\"Visualization:     generate_storyboard, generate_comparison_plot\")\n",
    "print(\"Training:          train_regression_models\")\n",
    "print(\"-\" * 50)\n",
    "print(\"All components ready.\")\n",
    "print(\"\\nOutput folders will contain:\")\n",
    "print(\"  - metrics/       : CSV results and summary\")\n",
    "print(\"  - intermediate/  : Cluster CSVs and filtered points (.npz)\")\n",
    "print(\"  - visualizations/: 4-panel storyboard PNGs\")\n",
    "print(\"  - models/        : Trained regression models (.pkl)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
